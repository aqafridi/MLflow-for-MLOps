{"cells":[{"cell_type":"markdown","source":["# Automated MLflow Hyperparameter Tuning\n\nIn this lab, you will learn to tune hyperparameters in Azure Databricks. This lab will cover the following exercise:\n- Exercise 1: Using Automated MLflow for hyperparameter tuning.\n\nTo upload the necessary data, please follow the instructions in the lab guide."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f62499ac-91d6-4d46-a696-f941a4b41d9a"}}},{"cell_type":"markdown","source":["## Attach notebook to your cluster\nBefore executing any cells in the notebook, you need to attach it to your cluster. Make sure that the cluster is running.\n\nIn the notebook's toolbar, select the drop down arrow next to Detached, and then select your cluster under Attach to.\n\nMake sure you run each cells in order."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce772839-e83d-4b7b-b283-d789d38de5f9"}}},{"cell_type":"markdown","source":["-sandbox\n## Exercise 1: Using Automated MLflow for hyperparameter tuning\nIn this exercise, you will perform hyperparameter tuning by using the automated MLflow libary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b3ba5c9-053f-49cf-b7ce-69c40826ead0"}}},{"cell_type":"markdown","source":["### Load the data\nIn this exercise, you will be using a dataset of real estate sales transactions to predict the price-per-unit of a property based on its features. The price-per-unit in this data is based on a unit measurement of 3.3 square meters\n\nThe data consists of the following variables:\n- **transaction_date** - the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)\n- **house_age** - the house age (in years)\n- **transit_distance** - the distance to the nearest light rail station (in meters)\n- **local_convenience_stores** - the number of convenience stores within walking distance\n- **latitude** - the geographic coordinate, latitude\n- **longitude** - the geographic coordinate, longitude\n- **price_per_unit** - house price of unit area (3.3 square meters) \n\n\nRun the following cell to load the table into a Spark dataframe and review the dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3b56ddd-c0c1-4c26-8ce9-ca49b1f7195f"}}},{"cell_type":"code","source":["dataset = spark.sql(\"select * from real_estate\")\ndisplay(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7550a64-4398-4fc1-8cfa-308e50350e68"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Train a linear regression model\nStart by performing a train/test split on the housing dataset and building a pipeline for linear regression.\n\nIn the cell below, a dataframe `housingDF` is created from the table you created before. The dataframe is then randomnly split into a training set that contains 80% of the data, and a test set that contains 20% of the original dataset. All columns except for the last one are then marked as features so that a Linear Regression model can be trained on the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf5d9c42-a595-4737-b54a-407cd357798a"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nhousingDF = dataset.dropna(subset=['price_per_unit'])\n\ntrainDF, testDF = housingDF.randomSplit([0.8, 0.2], seed=42)\n\nassembler = VectorAssembler(inputCols=housingDF.columns[:-1], outputCol=\"features\")\n\nlr = (LinearRegression()\n  .setLabelCol(\"price_per_unit\")\n  .setFeaturesCol(\"features\")\n)\n\npipeline = Pipeline(stages = [assembler, lr])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e362c5f4-2053-49bc-9224-3aa086056030"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the model parameters using the `.explainParams()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f072af25-7056-4a62-86ac-8e99f9c46826"}}},{"cell_type":"code","source":["print(lr.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45dc95aa-310d-44db-9056-74257935f470"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n`ParamGridBuilder()` allows us to string together all of the different possible hyperparameters we would like to test.  In this case, we can test the maximum number of iterations, whether we want to use an intercept with the y axis, and whether we want to standardize our features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a179cd6-68ca-4eef-a86c-75821f3dd0c2"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n  .addGrid(lr.maxIter, [1, 10, 100])\n  .addGrid(lr.fitIntercept, [True, False])\n  .addGrid(lr.standardization, [True, False])\n  .build()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cffd2db-1b7d-408a-bdc8-f8a1d7de0d69"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now `paramGrid` contains all of the combinations we will test in the next step.  Take a look at what it contains."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4786c880-2525-42d4-acab-096e53f263cf"}}},{"cell_type":"code","source":["paramGrid"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"456b1818-9890-4126-bcd2-9520da65e1ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Cross-Validation\n\nThere are a number of different ways of conducting cross-validation, allowing us to trade off between computational expense and model performance.  An exhaustive approach to cross-validation would include every possible split of the training set.  More commonly, _k_-fold cross-validation is used where the training dataset is divided into _k_ smaller sets, or folds.  A model is then trained on _k_-1 folds of the training data and the last fold is used to evaluate its performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03aaa4b6-67c2-4540-b2e7-7e1498dc554e"}}},{"cell_type":"markdown","source":["Create a `RegressionEvaluator()` to evaluate our grid search experiments and a `CrossValidator()` to build our models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1611082c-b40b-4825-82a2-4f74304733a2"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nevaluator = RegressionEvaluator(\n  labelCol = \"price_per_unit\", \n  predictionCol = \"prediction\"\n)\n\ncv = CrossValidator(\n  estimator = pipeline,             # Estimator (individual model or pipeline)\n  estimatorParamMaps = paramGrid,   # Grid of parameters to try (grid search)\n  evaluator=evaluator,              # Evaluator\n  numFolds = 3,                     # Set k to 3\n  seed = 42                         # Seed to make sure our results are the same if ran again\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"379f705e-05ff-453f-96e1-b961c17ec218"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nFit the `CrossValidator()`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This will train a large number of models.  If your cluster size is too small, it could take a while."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bf09a78-3336-4523-afde-0a2df462e6e2"}}},{"cell_type":"code","source":["cvModel = cv.fit(trainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a093ee8-620a-4986-a6b3-545bde0c6741"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the scores from the different experiments."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94983fa0-d4c3-45be-a33f-dd4ed22102a7"}}},{"cell_type":"code","source":["for params, score in zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics):\n  print(\"\".join([param.name+\"\\t\"+str(params[param])+\"\\t\" for param in params]))\n  print(\"\\tScore: {}\".format(score))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34999dc7-23df-43a7-9897-f2d9df058680"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can then access the best model using the `.bestModel` attribute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f9e305e-332e-4d86-a8a3-144e2d2c0b5f"}}},{"cell_type":"code","source":["bestModel = cvModel.bestModel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8949f76c-2333-4dac-bb29-8362eaf28ae8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To see the predictions of the best model on the test dataset, execute the code below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc418db5-4300-441a-b01d-2280e0e6a660"}}},{"cell_type":"code","source":["predictions = cvModel.bestModel.transform(testDF)\ndisplay(predictions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fed22697-6ed8-4557-9781-64027ca23819"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.0 Automated MLflow Hyperparameter Tuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2769267563032240}},"nbformat":4,"nbformat_minor":0}
